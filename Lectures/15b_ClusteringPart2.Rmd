---
title: "Clustering"
subtitle: "Part 2b"
author: "Prof. Weldzius"
institute: "Villanova University"
# date: "Slides Updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    # self_contained: true
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css:
      - default
      - css/lexis.css
      - css/lexis-fonts.css
    #seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"

---

```{css,echo = F}
.small .remark-code { /*Change made here*/
  font-size: 85% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}
```

```{r,include=F}
set.seed(123)
options(width=60)
knitr::opts_chunk$set(fig.align='center',fig.width=9,fig.height=5,message=F,warning=F)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

# Agenda

1. Tweets as data

2. Words &rarr; topics

3. Application

```{r}
require(tidyverse)
require(scales)
tweet_words <- read_rds(file="../Data/Trump_tweet_words.Rds")
```

---

# NLP Definitions

- Before we dig in, some important definitions

--
  
1. **Word / Term:** The core unit of interest

--

  - Often pre-processed to remove "stop words" and to "stem" the words
  
  - "Stop word": an uninteresting, commonly used word
  
  - "Stem / Lemmatize": the core component of a word that contains its meaning (eat, ate, eaten &rarr; eat)
  
--
  
2. **Document:** A collection of words with a single purpose / idea (i.e., a tweet, an essay)

--
  
3. **Corpus:** A collection of documents

--

4. **BOW:** Bag-of-words. Convert a **document** into a count of how many times **words** appear

--

5. **DTM:** Document-term matrix. A dataset where rows are **documents**, columns are **words**, and the values are the counts from **BOW**


---

# What does Trump tweet about?

- Core idea: word frequencies can help:

--

  - Help us understand **documents**
  
--

  - Which help us understand **authors**
  
```{r}
counts <- tweet_words %>% 
  count(word) %>%
  arrange(-n)
```

---

# What does Trump tweet about?

```{r}
counts
```

---

# What does Trump tweet about?

```{r}
p <- tweet_words %>%
  count(word, sort = TRUE) %>% # New ways of doing old things
  head(20) %>%
  ggplot(aes(x = n,y = reorder(word, n))) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  scale_x_continuous(label=comma)
```

---

# What does Trump tweet about?

```{r}
p
```


---

# Effect of becoming president

- Did his focus change when he became president?

```{r}
tweet_words <- tweet_words %>%
  mutate(PostPresident = Tweeting.date > as.Date("2016-11-03"))
```

--

```{r}
p <- tweet_words %>%
  count(PostPresident,word) %>%
  group_by(PostPresident) %>%
  arrange(-n) %>%
  slice(1:10) %>%
  ggplot(aes(x = n,y = reorder(word, n),
             fill = PostPresident)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  scale_x_continuous(label=comma)
```

---

# Effect of becoming president

```{r}
p
```

---

# Document Term Matrix

- "DTM" counts all the words in each document

--

- In this case, a "document" is a tweet

```{r}
dtm <- tweet_words %>%
  count(document,word)
glimpse(dtm)
```

---

# Document Term Matrix

- However, each tweet is very short

--

- Let's consider the `Tweeting.date` the document

--

  - Concept: What is Trump tweeting about on a given day?
  
```{r}
dtm <- tweet_words %>%
  count(Tweeting.date,word) %>%
  group_by(word) %>%
  mutate(tot_n = sum(n)) %>% # Also calculate TOTAL number of times a word appears
  ungroup()
```

---

# Wrangle

- Extremely rare words should be dropped (typos, etc.)

```{r}
dtm %>%
  arrange(tot_n) %>% head() # Trump tweeted "barnesandnoblecom" only once in his life

dtm <- dtm %>%
  filter(tot_n > 20) # Drop these rarely occurring words
```


---

# Analyzing BOW

- Some words are frequently found in many documents

--

- We want to find words that are **uniquely** used

--

  - "Unique" &rarr; used frequently in one document but not in any others
  
--

- "TF-IDF": Term frequency-inverse document frequency

--

- "TF": $\frac{\text{word count}}{\text{total words}}$

- "DF": $\frac{\text{documents with word}}{\text{total documents}}$

--

  - "IDF": Just invert it $\frac{\text{total documents}}{\text{documents with word}}$
  
$$tf-idf(w,d) = tf(w,d) \times log \left( \frac{N}{df(w)}\right) $$

---

# TF-IDF

```{r,warning=F,message=F}
require(tidytext) # Required to calculate TF-IDF
# install.packages("tm", repos="http://R-Forge.R-project.org") # may need to run this if the next chunk produces an error "there is no package called 'tm'"
require(tm)
dtm.tfidf <- bind_tf_idf(tbl = dtm, term = word, document = Tweeting.date, n = n) # Calculate TF-IDF
dtm.tfidf  %>%
  select(word,tf_idf) %>%
  distinct() %>%
  arrange(-tf_idf) %>%
  slice(1:10)
```

---

# $K$-means

- How to summarize this? $k$-means clustering!

--

- Recall that `kmeans()` function clusters over every column in a data frame

--

- `dtm.tfidf` is organized "long" (i.e., each row is a word-by-document)

--

- Want to convert to "wide" (i.e., rows are documents, columns are words)

--

- Previously used `spread()`, but for *k*-means with text: `cast_dtm()`

--

```{r}
castdtm <- tidytext::cast_dtm(data = dtm.tfidf, document = Tweeting.date, term = word, value = tf_idf)
```

--

- Now let's calculate `kmeans()` (this will take a few seconds)

```{r}
set.seed(42)
km_out <- kmeans(castdtm, 
                 centers = 50, # Number of "topics"
                 nstart = 5)
```

---

# Looking at Clusters

- Some quick wrangling

```{r,warning=F,message=F}
require(tidymodels)
km_out_tidy <- tidy(km_out) %>%
  gather(word,mean_tfidf,-size,-cluster,-withinss) %>% # Convert to long data
  mutate(mean_tfidf = as.numeric(mean_tfidf)) # Calculate average TF-IDF
km_out_tidy
```

---

# Looking at Clusters

- And can plot! (Just look at first 10 "topics")

```{r}
p <- km_out_tidy %>%
  filter(cluster %in% 1:9) %>%
  group_by(cluster) %>%
  arrange(-mean_tfidf) %>%
  slice(1:10) %>%
  ggplot(aes(x = mean_tfidf,y = reorder(word,mean_tfidf),
             fill = factor(cluster))) + 
  geom_bar(stat = 'identity') + 
  facet_wrap(~cluster,scales = 'free') + 
  labs(title = 'k-means Clusters',
       subtitle = 'Clustered by TF-IDF',
       x = 'Centroid',
       y = NULL,
       fill = 'Cluster ID')
```

---

# Looking at Clusters

```{r}
p
```

---

# Looking at clusters

- What are these clusters?

--

- Let's look at the most popular topics

```{r}
(tops <- km_out_tidy %>%
  select(size,withinss,cluster) %>%
  distinct() %>%
  arrange(desc(size)) %>%
    slice(1:5))
```

---

# Looking at clusters

- What are these clusters?

- Let's look at the most popular topics

```{r}
p <- km_out_tidy %>%
  filter(cluster %in% tops$cluster) %>%
  group_by(cluster) %>%
  arrange(-mean_tfidf) %>%
  slice(1:10) %>%
  ggplot(aes(x = mean_tfidf,y = reorder(word,mean_tfidf),
             fill = factor(cluster))) + 
  geom_bar(stat = 'identity') + 
  facet_wrap(~cluster,scales = 'free') + 
  labs(title = 'k-means Clusters',
       subtitle = 'Clustered by TF-IDF',
       x = 'Centroid',
       y = NULL,
       fill = 'Cluster ID')
```

---

# Looking at clusters

```{r}
p
```

---

# Optimal number of clusters?

- We know how to do this already! Elbow plot!

```{r}
set.seed(42)
totWSS <- NULL
for(k in c(1,10,50,100,250,500,1000)) {
  km_out <- kmeans(castdtm, 
                 centers = k,
                 nstart = 5)
  
  totWSS <- data.frame(totWSS = km_out$tot.withinss,
                       k = k) %>%
    bind_rows(totWSS)
}

p <- totWSS %>%
  ggplot(aes(x = k,y = totWSS)) + 
  geom_point() + 
  geom_line()
```

---

# Optimal number of clusters?

```{r}
p
```


---

# Conclusion

- $k$-means clustering on text &rarr; **topics**

--

- As always, this is a deep area of study

--

  - Superior methods are out there (Latent Dirichlet Allocation, Structural Topic Modeling, etc.)
  
--

- NOTE: even with text, always start with simple descriptives

--

  - **Looking** at your data is the heart of data science


